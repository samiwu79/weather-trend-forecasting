{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff36905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.11/site-packages (14.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%pip install pandas numpy matplotlib pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b74c4",
   "metadata": {},
   "source": [
    "## 1) Import libraries and define file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9b55e8-0069-49c6-8e92-cc0bdade4140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120468d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Raw CSV not found: data/GlobalWeatherRepository.csv. Please place the Kaggle file in the data/ folder.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m CLEAN \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/cleaned_weather.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)           \u001b[38;5;66;03m# Where the cleaned file will be saved\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Safety check: stop early if the file is missing\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m RAW\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw CSV not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRAW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please place the Kaggle file in the data/ folder.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw CSV path:\u001b[39m\u001b[38;5;124m\"\u001b[39m, RAW\u001b[38;5;241m.\u001b[39mresolve())\n",
      "\u001b[0;31mAssertionError\u001b[0m: Raw CSV not found: data/GlobalWeatherRepository.csv. Please place the Kaggle file in the data/ folder."
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd      \n",
    "import numpy as np       \n",
    "from pathlib import Path \n",
    "\n",
    "RAW = Path(\"data/GlobalWeatherRepository.csv\")   # Location of your raw CSV\n",
    "CLEAN = Path(\"data/cleaned_weather.csv\")           # Where the cleaned file will be saved\n",
    "\n",
    "# Safety check: stop early if the file is missing\n",
    "assert RAW.exists(), f\"Raw CSV not found: {RAW}. Please place the Kaggle file in the data/ folder.\"\n",
    "print(\"Raw CSV path:\", RAW.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d884236",
   "metadata": {},
   "source": [
    "## 2) Quick peek at the data (first 5 rows)\n",
    "\n",
    "**Explanation of each line below:**\n",
    "- `pd.read_csv(RAW, nrows=5, low_memory=False)` — Read only **5 rows** from the CSV to preview its structure.\n",
    "- `display(sample.head())` — Nicely show those 5 rows.\n",
    "- `print(\"Columns ...\")` — Print a list of the first 20 column names so you can see what's inside.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a728bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Global_Weather_Repository.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(RAW, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Load only 5 rows to preview\u001b[39;00m\n\u001b[1;32m      2\u001b[0m display(sample\u001b[38;5;241m.\u001b[39mhead())                                 \u001b[38;5;66;03m# Show the first 5 rows\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns (first 20):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(sample\u001b[38;5;241m.\u001b[39mcolumns)[:\u001b[38;5;241m20\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Global_Weather_Repository.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = pd.read_csv(RAW, nrows=5, low_memory=False)  # Load only 5 rows to preview\n",
    "display(sample.head())                                 # Show the first 5 rows\n",
    "print(\"Columns (first 20):\", list(sample.columns)[:20], \"...\")  # Show some column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa727f6c",
   "metadata": {},
   "source": [
    "## 3) Load the full dataset (24 MB is safe to load directly)\n",
    "\n",
    "**Explanation of each line below:**\n",
    "- `pd.read_csv(RAW, low_memory=False)` — Load the full CSV into a dataframe `df`.\n",
    "- `print(\"Shape:\", df.shape)` — Print how many rows and columns we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0840f6d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Global_Weather_Repository.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(RAW, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Load all rows\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape (rows, cols):\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Global_Weather_Repository.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(RAW, low_memory=False)  # Load all rows\n",
    "print(\"Shape (rows, cols):\", df.shape)   # Show dataset dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292384f",
   "metadata": {},
   "source": [
    "## 4) Normalize column names and parse the timestamp\n",
    "\n",
    "Different versions of the dataset may use different column names (e.g., `LastUpdated`, `lastupdated`, `date`).  \n",
    "We will:\n",
    "1. Make all column names **lowercase** and replace spaces with underscores (easier to type).\n",
    "2. Search a list of **candidate names** to find the timestamp column.\n",
    "3. Convert that column to a real time type (`datetime`) in **UTC** and call it `timestamp`.\n",
    "4. Drop rows where the time could not be parsed (rare but safe).\n",
    "5. Sort the table by time.\n",
    "\n",
    "**Explanation of each line below:**\n",
    "- `df.columns = [...]` — Create a new list of cleaned column names using a **list comprehension**.\n",
    "- `dt_candidates = [...]` — A list of possible time column names (we don’t know the exact spelling in your file).\n",
    "- `dt_col = next((c for c in dt_candidates if c in df.columns), None)` — Pick the first candidate that exists in your data.\n",
    "- `assert dt_col` — If none found, stop and instruct you to check the file.\n",
    "- `pd.to_datetime(..., errors=\"coerce\", utc=True)` — Convert text to a real datetime; bad values become `NaT` (missing time); use UTC timezone.\n",
    "- `dropna(subset=[\"timestamp\"])` — Remove rows with missing timestamps.\n",
    "- `sort_values(\"timestamp\")` — Order rows by time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72da7e47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Normalize column names: trim spaces, lowercase, replace spaces with underscores\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2) Try multiple possible time column names\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dt_candidates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlastupdated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlastupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Normalize column names: trim spaces, lowercase, replace spaces with underscores\n",
    "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# 2) Try multiple possible time column names\n",
    "dt_candidates = [\"lastupdated\",\"last_updated\",\"lastupdate\",\"timestamp\",\"date\",\"datetime\"]\n",
    "dt_col = next((c for c in dt_candidates if c in df.columns), None)\n",
    "assert dt_col, f\"No datetime column found. Tried: {dt_candidates}\"\n",
    "\n",
    "# 3) Parse to pandas datetime in UTC; invalid parses become NaT (missing)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[dt_col], errors=\"coerce\", utc=True)\n",
    "\n",
    "# 4) Drop rows with invalid/missing timestamps\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} rows with invalid timestamps.\")\n",
    "df[[\"timestamp\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9ffc3",
   "metadata": {},
   "source": [
    "## 5) Handle missing values (NaNs)\n",
    "\n",
    "We will fill missing values so later steps don’t break.\n",
    "\n",
    "**Strategy:**\n",
    "- For **numeric columns**: fill with the **median** (robust to outliers).\n",
    "- For **categorical/text columns**: fill with the **mode** (most frequent value).\n",
    "\n",
    "**Explanation of each line below:**\n",
    "- `select_dtypes(include=[np.number])` — Find numeric columns.\n",
    "- `cat_cols = [c for c ...]` — Everything else is treated as categorical.\n",
    "- Loop over the columns and fill missing values appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a290bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Identify numeric and categorical columns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns            \u001b[38;5;66;03m# numeric columns like temperature, precipitation, etc.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m cat_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m num_cols]             \u001b[38;5;66;03m# non-numeric columns like city, country\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Fill numeric NaNs with the median of that column\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns            # numeric columns like temperature, precipitation, etc.\n",
    "cat_cols = [c for c in df.columns if c not in num_cols]             # non-numeric columns like city, country\n",
    "\n",
    "# Fill numeric NaNs with the median of that column\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "# Fill categorical NaNs with the most frequent value (mode) of that column\n",
    "for c in cat_cols:\n",
    "    if df[c].isna().any():\n",
    "        mode = df[c].mode()\n",
    "        if not mode.empty:\n",
    "            df[c] = df[c].fillna(mode.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426190cb",
   "metadata": {},
   "source": [
    "## 6) Reduce the impact of extreme outliers\n",
    "\n",
    "We **clip** extreme values to a safe range (between the 1st and 99th percentile).  \n",
    "This keeps unusual spikes from dominating your analysis, while still keeping the row.\n",
    "\n",
    "**Explanation of each line below:**\n",
    "- `quantile([0.01, 0.99])` — Calculate the 1% and 99% cutoff points for a column.\n",
    "- `clip(lower=..., upper=...)` — Replace values below/above the cutoffs with the cutoff values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e194c839",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m num_cols:\n\u001b[1;32m      2\u001b[0m     q1, q99 \u001b[38;5;241m=\u001b[39m df[c]\u001b[38;5;241m.\u001b[39mquantile([\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.99\u001b[39m])  \u001b[38;5;66;03m# 1st and 99th percentiles\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     df[c] \u001b[38;5;241m=\u001b[39m df[c]\u001b[38;5;241m.\u001b[39mclip(lower\u001b[38;5;241m=\u001b[39mq1, upper\u001b[38;5;241m=\u001b[39mq99)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_cols' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for c in num_cols:\n",
    "    q1, q99 = df[c].quantile([0.01, 0.99])  # 1st and 99th percentiles\n",
    "    df[c] = df[c].clip(lower=q1, upper=q99) # Clip extreme values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7bf735",
   "metadata": {},
   "source": [
    "## 7) Aggregate to **daily** level (optionally per city/country)\n",
    "\n",
    "Your dataset may have multiple records inside the same day.  \n",
    "We will:\n",
    "1. Create a `date` column from `timestamp`.\n",
    "2. Define **grouping keys**: always `date`, plus `city`/`country` if those columns exist.\n",
    "3. Group and compute the **mean** for numeric columns.\n",
    "\n",
    "**Explanation of each line below:**\n",
    "- `df[\"timestamp\"].dt.date` — Extract the date (year-month-day) from the full timestamp.\n",
    "- `group_keys = [...]` — Start with `date`; add `city`/`country` if available.\n",
    "- `groupby(...).mean(numeric_only=True)` — Average numeric values inside the same group.\n",
    "- `reset_index()` — Turn the group keys back into normal columns.\n",
    "- Re-create `timestamp` from `date` so later plotting is easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97904e81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Create a pure date column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2) Build grouping keys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m group_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Create a pure date column\n",
    "df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "\n",
    "# 2) Build grouping keys\n",
    "group_keys = [\"date\"]\n",
    "if \"city\" in df.columns: group_keys.append(\"city\")\n",
    "if \"country\" in df.columns: group_keys.append(\"country\")\n",
    "\n",
    "# 3) Group and average numeric values\n",
    "df_daily = df.groupby(group_keys).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Recreate a timestamp column (midnight of that day)\n",
    "import pandas as pd\n",
    "df_daily[\"timestamp\"] = pd.to_datetime(df_daily[\"date\"])  # converts 'date' back to a datetime\n",
    "df_daily = df_daily.drop(columns=[\"date\"])\n",
    "\n",
    "print(\"Daily shape:\", df_daily.shape)\n",
    "df_daily.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6cf84",
   "metadata": {},
   "source": [
    "## 8) Save the cleaned dataset\n",
    "\n",
    "We save the result to `data/cleaned_weather.csv`.  \n",
    "This file will be used by EDA and forecasting notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02914ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_daily' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ensure the output folder exists; then write CSV\u001b[39;00m\n\u001b[1;32m      2\u001b[0m CLEAN\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m df_daily\u001b[38;5;241m.\u001b[39mto_csv(CLEAN, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cleaned dataset to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, CLEAN\u001b[38;5;241m.\u001b[39mresolve())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_daily' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure the output folder exists; then write CSV\n",
    "CLEAN.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_daily.to_csv(CLEAN, index=False)\n",
    "print(\"Saved cleaned dataset to:\", CLEAN.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b409f",
   "metadata": {},
   "source": [
    "## 9) (Optional) Quick visual check (matplotlib only)\n",
    "\n",
    "We try to plot **temperature** and **precipitation** trends for one city, if such columns exist.\n",
    "\n",
    "- We **do not** use seaborn here (to keep it simple and dependency-light).\n",
    "- Each chart is drawn in its **own figure**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef25b8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_daily' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m precip_candidates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecip_mm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecipitation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrain_mm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Detect which columns exist in your cleaned daily data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m temp_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m temp_candidates \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_daily\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m precip_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m precip_candidates \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_daily\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# If we have a city and temperature column, pick the most frequent city and plot its trends\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m precip_candidates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecip_mm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecipitation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrain_mm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Detect which columns exist in your cleaned daily data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m temp_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m temp_candidates \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_daily\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m precip_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m precip_candidates \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_daily\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# If we have a city and temperature column, pick the most frequent city and plot its trends\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_daily' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Candidate column names that commonly appear in this dataset\n",
    "temp_candidates = [\"temp_c\",\"temperature_c\",\"temperature\",\"avg_temp_c\",\"temp\"]\n",
    "precip_candidates = [\"precip_mm\",\"precipitation\",\"rain_mm\",\"rain\"]\n",
    "\n",
    "# Detect which columns exist in your cleaned daily data\n",
    "temp_col = next((c for c in temp_candidates if c in df_daily.columns), None)\n",
    "precip_col = next((c for c in precip_candidates if c in df_daily.columns), None)\n",
    "\n",
    "# If we have a city and temperature column, pick the most frequent city and plot its trends\n",
    "if \"city\" in df_daily.columns and temp_col:\n",
    "    city = df_daily[\"city\"].astype(str).value_counts().index[0]\n",
    "    sub = df_daily[df_daily[\"city\"]==city].sort_values(\"timestamp\")\n",
    "    \n",
    "    # Temperature trend\n",
    "    plt.figure()\n",
    "    plt.plot(sub[\"timestamp\"], sub[temp_col])\n",
    "    plt.title(f\"Temperature trend — {city}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(temp_col)\n",
    "    plt.show()\n",
    "    \n",
    "    # Precipitation trend (if available)\n",
    "    if precip_col:\n",
    "        plt.figure()\n",
    "        plt.plot(sub[\"timestamp\"], sub[precip_col])\n",
    "        plt.title(f\"Precipitation trend — {city}\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(precip_col)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Skipping quick plots: missing 'city' or temperature column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ca2e9-3393-4d48-b2d2-44b407bf3d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66f976-005f-4746-87a1-425c089f9156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021c1dc-dcc5-4c00-b82c-fff514bd6d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
