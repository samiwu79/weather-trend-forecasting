{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weather Trend Forecasting — Data Cleaning (Beginner Friendly)\n\n**CSV size:** ~24 MB → normal loading is fine.\n\nThis notebook teaches you, step by step, how to clean the Kaggle dataset **Global Weather Repository**.\n\nWe strictly separate **explanations (Markdown cells like this one)** from **runnable Python code (Code cells)**.\n\n### What you will do\n1. Load the raw CSV\n2. Inspect columns and a small sample\n3. Normalize column names\n4. Find and parse the timestamp (using `lastupdated` or similar)\n5. Handle missing values (numeric & categorical)\n6. Clip extreme outliers\n7. Aggregate to **daily level** (per city/country if available)\n8. Save the cleaned CSV for EDA/modeling\n\n**Inputs**: `data/Global_Weather_Repository.csv`  \n**Outputs**: `data/cleaned_weather.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) (Optional) Install dependencies\n\nIf your environment is missing packages, remove the `#` and run the cell once.\n\n- `pandas` handles tables\n- `numpy` helps with numbers\n- `matplotlib` draws charts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n# %pip install pandas numpy matplotlib pyarrow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Import libraries and define file paths\n\n**Explanation of each line below:**\n- `import pandas as pd` — Load the **pandas** library and give it a short name `pd`. We'll use `pd.read_csv(...)` and so on.\n- `import numpy as np` — Load **numpy** as `np` for numeric helpers.\n- `from pathlib import Path` — `Path` is a simple way to work with file system paths (like `data/myfile.csv`).\n- `RAW = Path(\"...\")` — Define where the raw CSV lives. We use a Path object instead of a plain string.\n- `CLEAN = Path(\"...\")` — Define where the cleaned CSV should be saved.\n- `assert RAW.exists(), \"...\"` — If the file doesn’t exist, **stop** and show a helpful message.\n- `print(\"Raw CSV path:\", RAW.resolve())` — Show the absolute path, so you know what file we are reading.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\nimport pandas as pd      # Data handling\nimport numpy as np       # Numeric helpers\nfrom pathlib import Path # File path utilities\n\nRAW = Path(\"data/Global_Weather_Repository.csv\")   # Location of your raw CSV\nCLEAN = Path(\"data/cleaned_weather.csv\")           # Where the cleaned file will be saved\n\n# Safety check: stop early if the file is missing\nassert RAW.exists(), f\"Raw CSV not found: {RAW}. Please place the Kaggle file in the data/ folder.\"\nprint(\"Raw CSV path:\", RAW.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Quick peek at the data (first 5 rows)\n\n**Explanation of each line below:**\n- `pd.read_csv(RAW, nrows=5, low_memory=False)` — Read only **5 rows** from the CSV to preview its structure.\n- `display(sample.head())` — Nicely show those 5 rows.\n- `print(\"Columns ...\")` — Print a list of the first 20 column names so you can see what's inside.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\nsample = pd.read_csv(RAW, nrows=5, low_memory=False)  # Load only 5 rows to preview\ndisplay(sample.head())                                 # Show the first 5 rows\nprint(\"Columns (first 20):\", list(sample.columns)[:20], \"...\")  # Show some column names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load the full dataset (24 MB is safe to load directly)\n\n**Explanation of each line below:**\n- `pd.read_csv(RAW, low_memory=False)` — Load the full CSV into a dataframe `df`.\n- `print(\"Shape:\", df.shape)` — Print how many rows and columns we have.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\ndf = pd.read_csv(RAW, low_memory=False)  # Load all rows\nprint(\"Shape (rows, cols):\", df.shape)   # Show dataset dimensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Normalize column names and parse the timestamp\n\nDifferent versions of the dataset may use different column names (e.g., `LastUpdated`, `lastupdated`, `date`).  \nWe will:\n1. Make all column names **lowercase** and replace spaces with underscores (easier to type).\n2. Search a list of **candidate names** to find the timestamp column.\n3. Convert that column to a real time type (`datetime`) in **UTC** and call it `timestamp`.\n4. Drop rows where the time could not be parsed (rare but safe).\n5. Sort the table by time.\n\n**Explanation of each line below:**\n- `df.columns = [...]` — Create a new list of cleaned column names using a **list comprehension**.\n- `dt_candidates = [...]` — A list of possible time column names (we don’t know the exact spelling in your file).\n- `dt_col = next((c for c in dt_candidates if c in df.columns), None)` — Pick the first candidate that exists in your data.\n- `assert dt_col` — If none found, stop and instruct you to check the file.\n- `pd.to_datetime(..., errors=\"coerce\", utc=True)` — Convert text to a real datetime; bad values become `NaT` (missing time); use UTC timezone.\n- `dropna(subset=[\"timestamp\"])` — Remove rows with missing timestamps.\n- `sort_values(\"timestamp\")` — Order rows by time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n# 1) Normalize column names: trim spaces, lowercase, replace spaces with underscores\ndf.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n\n# 2) Try multiple possible time column names\ndt_candidates = [\"lastupdated\",\"last_updated\",\"lastupdate\",\"timestamp\",\"date\",\"datetime\"]\ndt_col = next((c for c in dt_candidates if c in df.columns), None)\nassert dt_col, f\"No datetime column found. Tried: {dt_candidates}\"\n\n# 3) Parse to pandas datetime in UTC; invalid parses become NaT (missing)\ndf[\"timestamp\"] = pd.to_datetime(df[dt_col], errors=\"coerce\", utc=True)\n\n# 4) Drop rows with invalid/missing timestamps\nbefore = len(df)\ndf = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\nafter = len(df)\nprint(f\"Dropped {before - after} rows with invalid timestamps.\")\ndf[[\"timestamp\"]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Handle missing values (NaNs)\n\nWe will fill missing values so later steps don’t break.\n\n**Strategy:**\n- For **numeric columns**: fill with the **median** (robust to outliers).\n- For **categorical/text columns**: fill with the **mode** (most frequent value).\n\n**Explanation of each line below:**\n- `select_dtypes(include=[np.number])` — Find numeric columns.\n- `cat_cols = [c for c ...]` — Everything else is treated as categorical.\n- Loop over the columns and fill missing values appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n# Identify numeric and categorical columns\nnum_cols = df.select_dtypes(include=[np.number]).columns            # numeric columns like temperature, precipitation, etc.\ncat_cols = [c for c in df.columns if c not in num_cols]             # non-numeric columns like city, country\n\n# Fill numeric NaNs with the median of that column\nfor c in num_cols:\n    if df[c].isna().any():\n        df[c] = df[c].fillna(df[c].median())\n\n# Fill categorical NaNs with the most frequent value (mode) of that column\nfor c in cat_cols:\n    if df[c].isna().any():\n        mode = df[c].mode()\n        if not mode.empty:\n            df[c] = df[c].fillna(mode.iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Reduce the impact of extreme outliers\n\nWe **clip** extreme values to a safe range (between the 1st and 99th percentile).  \nThis keeps unusual spikes from dominating your analysis, while still keeping the row.\n\n**Explanation of each line below:**\n- `quantile([0.01, 0.99])` — Calculate the 1% and 99% cutoff points for a column.\n- `clip(lower=..., upper=...)` — Replace values below/above the cutoffs with the cutoff values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\nfor c in num_cols:\n    q1, q99 = df[c].quantile([0.01, 0.99])  # 1st and 99th percentiles\n    df[c] = df[c].clip(lower=q1, upper=q99) # Clip extreme values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Aggregate to **daily** level (optionally per city/country)\n\nYour dataset may have multiple records inside the same day.  \nWe will:\n1. Create a `date` column from `timestamp`.\n2. Define **grouping keys**: always `date`, plus `city`/`country` if those columns exist.\n3. Group and compute the **mean** for numeric columns.\n\n**Explanation of each line below:**\n- `df[\"timestamp\"].dt.date` — Extract the date (year-month-day) from the full timestamp.\n- `group_keys = [...]` — Start with `date`; add `city`/`country` if available.\n- `groupby(...).mean(numeric_only=True)` — Average numeric values inside the same group.\n- `reset_index()` — Turn the group keys back into normal columns.\n- Re-create `timestamp` from `date` so later plotting is easy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n# 1) Create a pure date column\ndf[\"date\"] = df[\"timestamp\"].dt.date\n\n# 2) Build grouping keys\ngroup_keys = [\"date\"]\nif \"city\" in df.columns: group_keys.append(\"city\")\nif \"country\" in df.columns: group_keys.append(\"country\")\n\n# 3) Group and average numeric values\ndf_daily = df.groupby(group_keys).mean(numeric_only=True).reset_index()\n\n# Recreate a timestamp column (midnight of that day)\nimport pandas as pd\ndf_daily[\"timestamp\"] = pd.to_datetime(df_daily[\"date\"])  # converts 'date' back to a datetime\ndf_daily = df_daily.drop(columns=[\"date\"])\n\nprint(\"Daily shape:\", df_daily.shape)\ndf_daily.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Save the cleaned dataset\n\nWe save the result to `data/cleaned_weather.csv`.  \nThis file will be used by EDA and forecasting notebooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n# Ensure the output folder exists; then write CSV\nCLEAN.parent.mkdir(parents=True, exist_ok=True)\ndf_daily.to_csv(CLEAN, index=False)\nprint(\"Saved cleaned dataset to:\", CLEAN.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) (Optional) Quick visual check (matplotlib only)\n\nWe try to plot **temperature** and **precipitation** trends for one city, if such columns exist.\n\n- We **do not** use seaborn here (to keep it simple and dependency-light).\n- Each chart is drawn in its **own figure**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\nimport matplotlib.pyplot as plt\n\n# Candidate column names that commonly appear in this dataset\ntemp_candidates = [\"temp_c\",\"temperature_c\",\"temperature\",\"avg_temp_c\",\"temp\"]\nprecip_candidates = [\"precip_mm\",\"precipitation\",\"rain_mm\",\"rain\"]\n\n# Detect which columns exist in your cleaned daily data\ntemp_col = next((c for c in temp_candidates if c in df_daily.columns), None)\nprecip_col = next((c for c in precip_candidates if c in df_daily.columns), None)\n\n# If we have a city and temperature column, pick the most frequent city and plot its trends\nif \"city\" in df_daily.columns and temp_col:\n    city = df_daily[\"city\"].astype(str).value_counts().index[0]\n    sub = df_daily[df_daily[\"city\"]==city].sort_values(\"timestamp\")\n    \n    # Temperature trend\n    plt.figure()\n    plt.plot(sub[\"timestamp\"], sub[temp_col])\n    plt.title(f\"Temperature trend — {city}\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(temp_col)\n    plt.show()\n    \n    # Precipitation trend (if available)\n    if precip_col:\n        plt.figure()\n        plt.plot(sub[\"timestamp\"], sub[precip_col])\n        plt.title(f\"Precipitation trend — {city}\")\n        plt.xlabel(\"Date\")\n        plt.ylabel(precip_col)\n        plt.show()\nelse:\n    print(\"Skipping quick plots: missing 'city' or temperature column.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}